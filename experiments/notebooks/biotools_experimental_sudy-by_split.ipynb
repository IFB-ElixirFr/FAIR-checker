{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "fc_src = \"../..\"\n",
    "sys.path.insert(0, fc_src)\n",
    "\n",
    "import requests\n",
    "from os import path\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "from rdflib import ConjunctiveGraph, Namespace, URIRef\n",
    "from rdflib.namespace import RDF, RDFS\n",
    "schema = Namespace(\"http://schema.org/\")\n",
    "\n",
    "from metrics.WebResource import WebResource\n",
    "from metrics.FAIRMetricsFactory import FAIRMetricsFactory\n",
    "from metrics.AbstractFAIRMetrics import AbstractFAIRMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exec_time_df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "def eval_metrics(web_res):\n",
    "    metrics_collection = []\n",
    "    metrics_collection.append(FAIRMetricsFactory.get_F1A(web_res))\n",
    "    metrics_collection.append(FAIRMetricsFactory.get_F1B(web_res))\n",
    "    metrics_collection.append(FAIRMetricsFactory.get_F2A(web_res))\n",
    "#    metrics_collection.append(FAIRMetricsFactory.get_F2B_weak(web_res))\n",
    "    metrics_collection.append(FAIRMetricsFactory.get_I1A(web_res))\n",
    "    metrics_collection.append(FAIRMetricsFactory.get_I1B(web_res))\n",
    "    metrics_collection.append(FAIRMetricsFactory.get_I2A(web_res))\n",
    "    metrics_collection.append(FAIRMetricsFactory.get_I2B(web_res))\n",
    "    metrics_collection.append(FAIRMetricsFactory.get_I3(web_res))\n",
    "    metrics_collection.append(FAIRMetricsFactory.get_R11(web_res))\n",
    "    metrics_collection.append(FAIRMetricsFactory.get_R12(web_res))\n",
    "#    metrics_collection.append(FAIRMetricsFactory.get_R13(web_res))\n",
    "    \n",
    "    row = {\"ID\":web_res.get_url()}\n",
    "    row_time = {\"ID\":web_res.get_url()}\n",
    "    for m in metrics_collection:\n",
    "        ts1 = time.time()\n",
    "        e = m.evaluate()\n",
    "        duration = round((time.time() - ts1), 2)\n",
    "        if e is not None:\n",
    "            row[m.get_principle_tag()] = e.get_score()\n",
    "            row_time[m.get_principle_tag()] = duration\n",
    "    \n",
    "    return row, row_time\n",
    "\n",
    "def mass_eval(samples):\n",
    "    evals = []\n",
    "    exec_time = []\n",
    "\n",
    "    for url, graph in tqdm(samples.items()):\n",
    "        wr = WebResource(url=url, rdf_graph=graph)\n",
    "        row, row_time = eval_metrics(wr)\n",
    "        evals.append(row)\n",
    "        exec_time.append(row_time)\n",
    "        \n",
    "    return evals, exec_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_dump():\n",
    "    for i in tqdm(index.keys()):\n",
    "        sub_graph = ConjunctiveGraph()\n",
    "        for s, p, o in KG.triples((URIRef(i), None, None)):\n",
    "            sub_graph.add((s, p, o))\n",
    "        index[i]=sub_graph\n",
    "\n",
    "def get_RDF_sparql(bio_tools_Id):\n",
    "    q = f\"CONSTRUCT {{<{bio_tools_Id}> ?p ?o}} WHERE {{<{bio_tools_Id}> rdf:type schema:SoftwareApplication . <{bio_tools_Id}> ?p ?o .}}\"\n",
    "    res = KG.query(q)\n",
    "    print(res.serialize(format=\"turtle\"))\n",
    "\n",
    "def get_RDF(bio_tools_Id):\n",
    "    sub_graph = ConjunctiveGraph()\n",
    "    for s, p, o in KG.triples((URIRef(bio_tools_Id), None, None)):\n",
    "        sub_graph.add((s, p, o))\n",
    "    print(sub_graph.serialize(format=\"turtle\"))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b076f5d55dd42baa3ba433c4409e195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./split_10.ttl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b10133c6ce4de8b9698d9bfadbe851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55af74b640746af9c015aab211f3164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "input_files = glob.glob(\"./split_*.ttl\")\n",
    "\n",
    "i = 0\n",
    "for filename in tqdm(input_files):\n",
    "    print(filename)\n",
    "    \n",
    "    KG = ConjunctiveGraph()\n",
    "    KG.parse(filename, format=\"turtle\")\n",
    "    \n",
    "    #index of biotools {IDs: rdf KG} \n",
    "    index = {}\n",
    "    for s, p, o in KG.triples((None, RDF.type, schema.SoftwareApplication)):\n",
    "        index[str(s)] = None    \n",
    "    \n",
    "    for bio_tools_Id in tqdm(index.keys()):\n",
    "        sub_graph = ConjunctiveGraph()\n",
    "        for s, p, o in KG.triples((URIRef(bio_tools_Id), None, None)):\n",
    "            sub_graph.add((s, p, o))\n",
    "        index[bio_tools_Id]=sub_graph\n",
    "    \n",
    "    # for each index, FAIR evaluation of all entries\n",
    "    df = pd.DataFrame()\n",
    "    df_time = pd.DataFrame()\n",
    "    \n",
    "    evals, exec_time = mass_eval(index)\n",
    "    df = pd.concat([df, pd.DataFrame.from_records(evals)])\n",
    "    df_time = pd.concat([df_time, pd.DataFrame.from_records(exec_time)])\n",
    "    \n",
    "    df.to_csv(\"FC_results_\"+str(i)+\".csv\")\n",
    "    df_time.to_csv(\"exec_time\"+str(i)+\".csv\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e202b6cb132b9357ed023fecdbc194f9a7f520726b3b39fdf8a2279e78a83e12"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
